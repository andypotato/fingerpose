<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="utf-8"/>
  <title>FingerPose Example</title>

  <!-- Require the peer dependencies of handpose. -->
  <script src="https://unpkg.com/@tensorflow/tfjs-core@3.7.0/dist/tf-core.js"></script>
  <script src="https://unpkg.com/@tensorflow/tfjs-converter@3.7.0/dist/tf-converter.js"></script>

  <!-- You must explicitly require a TF.js backend if you're not using the tfs union bundle. -->
  <script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@3.7.0/dist/tf-backend-webgl.js"></script>

  <!-- The main handpose library -->
  <script src="https://unpkg.com/@tensorflow-models/handpose@0.0.7/dist/handpose.js"></script>

  <!-- The fingerpose library -->
  <script src="fingerpose.js" type="text/javascript"></script>

  <style>

    * {
      box-sizing: border-box;
      user-select: none;
    }

    html, body {
      width: 100%;
      height: 100%;
      overflow: hidden;
      font-family: Arial, sans-serif;
      background-color: #ffffff;
      color: #333333;
    }

    body {
      margin: 0;
      padding: 0;
    }

    .container {
      margin: 20px auto;
      display: flex;
    }

    .video,
    .debug {
      padding: 0 20px;
    }

    table.summary {
      border: 1px solid #333;
      border-collapse: collapse;
    }

    table.summary td,
    table.summary th {
      border: 1px solid #333;
      padding: 5px 8px;
    }

    #video-container {
      width: 640px;
      height: 480px;
      position: relative;
    }

    .layer {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    #pose-video {
      transform: scaleX(-1);
    }

    #pose-result {
      font-size: 100px;
      text-align: right;
      padding: 20px 30px 0 0;
    }
  </style>
</head>
<body>

  <div class="container">

    <div class="video">
      <div id="video-container">
        <video id="pose-video" class="layer" playsinline></video>
        <canvas id="pose-canvas" class="layer"></canvas>
        <div id="pose-result" class="layer"></div>
      </div>
    </div>

    <div class="debug">
      <table class="summary">
        <thead>
          <tr>
            <th>Idx</th>
            <th>Finger</th>
            <th style="width: 110px">Curl</th>
            <th style="width: 170px">Direction</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>0</td>
            <td>Thumb</td>
            <td><span id="curl-0">-</span></td>
            <td><span id="dir-0">-</span></td>
          </tr>
          <tr>
            <td>1</td>
            <td>Index</td>
            <td><span id="curl-1">-</span></td>
            <td><span id="dir-1">-</span></td>
          </tr>
          <tr>
            <td>2</td>
            <td>Middle</td>
            <td><span id="curl-2">-</span></td>
            <td><span id="dir-2">-</span></td>
          </tr>
          <tr>
            <td>3</td>
            <td>Ring</td>
            <td><span id="curl-3">-</span></td>
            <td><span id="dir-3">-</span></td>
          </tr>
          <tr>
            <td>4</td>
            <td>Pinky</td>
            <td><span id="curl-4">-</span></td>
            <td><span id="dir-4">-</span></td>
          </tr>
        </tbody>
      </table>
    </div>

  </div>

  <script>

    const config = {
      video: { width: 640, height: 480, fps: 30 }
    };

    const landmarkColors = {
      thumb: 'red',
      indexFinger: 'blue',
      middleFinger: 'yellow',
      ringFinger: 'green',
      pinky: 'pink',
      palmBase: 'white'
    };

    const gestureStrings = {
      'thumbs_up': 'ðŸ‘',
      'victory': 'âœŒðŸ»'
    };

    async function main() {

      const video = document.querySelector("#pose-video");
      const canvas = document.querySelector("#pose-canvas");
      const ctx = canvas.getContext("2d");

      const resultLayer = document.querySelector("#pose-result");

      // configure gesture estimator
      // add "âœŒðŸ»" and "ðŸ‘" as sample gestures
      const knownGestures = [
        fp.Gestures.VictoryGesture,
        fp.Gestures.ThumbsUpGesture
      ];
      const GE = new fp.GestureEstimator(knownGestures);

      // load handpose model
      const model = await handpose.load();
      console.log("Handpose model loaded");

      // main estimation loop
      const estimateHands = async () => {

        // clear canvas overlay
        ctx.clearRect(0, 0, config.video.width, config.video.height);
        resultLayer.innerText = '';

        // get hand landmarks from video
        // Note: Handpose currently only detects one hand at a time
        // Therefore the maximum number of predictions is 1
        const predictions = await model.estimateHands(video, true);

        for(let i = 0; i < predictions.length; i++) {

          // draw colored dots at each predicted joint position
          for(let part in predictions[i].annotations) {
            for(let point of predictions[i].annotations[part]) {
              drawPoint(ctx, point[0], point[1], 3, landmarkColors[part]);
            }
          }

          // estimate gestures based on landmarks
          // using a minimum score of 9 (out of 10)
          // gesture candidates with lower score will not be returned
          const est = GE.estimate(predictions[i].landmarks, 9);

          if(est.gestures.length > 0) {

            // find gesture with highest match score
            let result = est.gestures.reduce((p, c) => { 
              return (p.score > c.score) ? p : c;
            });

            resultLayer.innerText = gestureStrings[result.name];
          }

          // update debug info
          updateDebugInfo(est.poseData);
        }

        // ...and so on
        setTimeout(() => { estimateHands(); }, 1000 / config.video.fps);
      };

      estimateHands();
      console.log("Starting predictions");
    }

    async function initCamera(width, height, fps) {

      const constraints = {
        audio: false,
        video: {
          facingMode: "user",
          width: width,
          height: height,
          frameRate: { max: fps }
        }
      };

      const video = document.querySelector("#pose-video");
      video.width = width;
      video.height = height;

      // get video stream
      const stream = await navigator.mediaDevices.getUserMedia(constraints);
      video.srcObject = stream;

      return new Promise(resolve => {
        video.onloadedmetadata = () => { resolve(video) };
      });
    }

    function drawPoint(ctx, x, y, r, color) {
      ctx.beginPath();
      ctx.arc(x, y, r, 0, 2 * Math.PI);
      ctx.fillStyle = color;
      ctx.fill();
    }

    function updateDebugInfo(data) {
      for(let fingerIdx in data) {
        document.getElementById("curl-" + fingerIdx).innerText = data[fingerIdx][1];
        document.getElementById("dir-" + fingerIdx).innerText = data[fingerIdx][2];
      }
    }

    window.addEventListener("DOMContentLoaded", () => {

      initCamera(
        config.video.width, config.video.height, config.video.fps
      ).then(video => {
        video.play();
        video.addEventListener("loadeddata", event => {
          console.log("Camera is ready");
          main();
        });
      });

      const canvas = document.querySelector("#pose-canvas");
      canvas.width = config.video.width;
      canvas.height = config.video.height;
      console.log("Canvas initialized");
    });
  </script>

</body>
</html>
